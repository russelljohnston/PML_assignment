---
title: "Practical Machine Learning - Prediction Assignment"
author: "Russell Johnston"
date: "24 December 2015"
output: html_document
---
<style type="text/css">

body, td {
   font-size: 12px;
}
code.r{
  font-size: 10px;
}
pre {
  font-size: 8px
}
</style>

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har).

## The Goal

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

## Libraries and Settings

We use the following libraries to perfom our analysis.

```{r eval=FALSE}
library(caret)
library(randomForest)
library(gbm)
library(survival)
library(splines)
library(parallel)

```

We set the seed to be 
```{r eval=FALSE}
set.seed(222)
```


## The Data

The training and testing data were respectively  downloaded from:

- [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

- [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

When reading in both datasets we standardise the data by setting any missing values  or `div/0`  to `NA` e.g.

```{r eval=FALSE}
train = read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
```


### Training Sample Selection

The training set contains a total of 19622 observations and 160 variables. Not all of these are predictor variables and we firstly remove columns: `X` (row number), `user_name`, `raw_timestamp_part_1`, `raw_timestamp_part_2` and `cvtd_timestamp`:

```{r eval=FALSE}
train <- train[-c(1:6)]
```

Just for conveneince we move the outcome variable `classe` to the first column:
```{r eval=FALSE}
train <- train[c(154,1:153)]
```

We found that there are predictor columns with a significant number of missing data. Figure 1 below shows a bar plot of the percentage of NA counts for each variable in the training set:

```{r eval=FALSE}
na_count <-100*(sapply(train, function(y) sum(length(which(is.na(y)))))/nrow(train))
barplot(as.matrix(t(na_count)), las=3,cex.names=0.4,ylab="% of 'NA' counts")
```

<figure>
<img   style="margin:30px 0px 0px 0px;" src="barplot.png" width="500px"/>
<figcaption style="font-size:10px; margin:0px 0px 20px 0px"> Figure 1.Bar plot showing the percentage of NA counts for each variable in the training set. </figcaption>
</figure>
As we can see all variables with missing data have at least 97.93% without recorded values. That is,  these variables have at most, 406 recorded observations out of 19,622.


```{r eval=FALSE}
min(na_count[na_count>0])
[1] 97.93089
```
As such these variables would  have minimal contribution to the training analysis and so we remove these from our analysis:

```{r eval=FALSE}
train <- train[,(na_count==0)]
```

This results in the removal of 100 further variables from our anaylsis giving a total of 53 predictors:
```
 [1] "classe"               "num_window"           "roll_belt"            "pitch_belt"           "yaw_belt"           
 [6] "total_accel_belt"     "gyros_belt_x"         "gyros_belt_y"         "gyros_belt_z"         "accel_belt_x"       
[11] "accel_belt_y"         "accel_belt_z"         "magnet_belt_x"        "magnet_belt_y"        "magnet_belt_z"      
[16] "roll_arm"             "pitch_arm"            "yaw_arm"              "total_accel_arm"      "gyros_arm_x"        
[21] "gyros_arm_y"          "gyros_arm_z"          "accel_arm_x"          "accel_arm_y"          "accel_arm_z"        
[26] "magnet_arm_x"         "magnet_arm_y"         "magnet_arm_z"         "roll_dumbbell"        "pitch_dumbbell"     
[31] "yaw_dumbbell"         "total_accel_dumbbell" "gyros_dumbbell_x"     "gyros_dumbbell_y"     "gyros_dumbbell_z"   
[36] "accel_dumbbell_x"     "accel_dumbbell_y"     "accel_dumbbell_z"     "magnet_dumbbell_x"    "magnet_dumbbell_y"  
[41] "magnet_dumbbell_z"    "roll_forearm"         "pitch_forearm"        "yaw_forearm"          "total_accel_forearm"
[46] "gyros_forearm_x"      "gyros_forearm_y"      "gyros_forearm_z"      "accel_forearm_x"      "accel_forearm_y"    
[51] "accel_forearm_z"      "magnet_forearm_x"     "magnet_forearm_y"     "magnet_forearm_z"  
```

We apply the same cuts to the  testing data set as above - this ensures certain fitting models used in the later analysis throw no errors due to missing values:

```{r eval=FALSE}
test <- test[-c(1:6)]
test <- test[c(154,1:153)]
test <- test[,(na_count==0)]
```

### Pre processing with PCA and Cross Validation

#### Principle Component Analysis (PCA)
Since this is quite a high dimensional dataset we explore by how much the predictor variables are correlated.  We do this by applying the `cor` function and isolate variables that have a correlation coefficient > 0.8:

```{r eval=FALSE}
corrTrain <- abs(cor(train[,-1]))
diag(corrTrain) <- 0
which(corrTrain>0.8,arr.ind=T)
```
```
                 row col
yaw_belt           4   2
total_accel_belt   5   2
accel_belt_y      10   2
accel_belt_z      11   2
accel_belt_x       9   3
magnet_belt_x     12   3
roll_belt          2   4
roll_belt          2   5
accel_belt_y      10   5
accel_belt_z      11   5
pitch_belt         3   9
magnet_belt_x     12   9
roll_belt          2  10
total_accel_belt   5  10
accel_belt_z      11  10
roll_belt          2  11
total_accel_belt   5  11
accel_belt_y      10  11
pitch_belt         3  12
accel_belt_x       9  12
gyros_arm_y       20  19
gyros_arm_x       19  20
magnet_arm_x      25  22
accel_arm_x       22  25
magnet_arm_z      27  26
magnet_arm_y      26  27
accel_dumbbell_x  35  29
accel_dumbbell_z  37  30
gyros_dumbbell_z  34  32
gyros_forearm_z   47  32
gyros_dumbbell_x  32  34
gyros_forearm_z   47  34
pitch_dumbbell    29  35
yaw_dumbbell      30  37
gyros_forearm_z   47  46
gyros_dumbbell_x  32  47
gyros_dumbbell_z  34  47
gyros_forearm_y   46  47

```

We can see we have a number of  predictors that are highly correlated which would make principal component analysis (PCA) a useful addition to the analysis as it will reduce the number of predictors and help optimise the compuational cost. We will use the `caret` package to perform the training analysis and pass through pre processing options via the `trainControl` function. In terms of PCA we set  `preProcOptions="pca"` which, as default, retains 95% variance. 
This is equivalent to 26 components:

```{r eval=FALSE}
 preProcess(train, method = "pca", thresh = 0.95)$numComp
```

#### Cross Validation

Implementing k-fold cross-validation will take the training sample and  randomly partition it into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The number of folds one chooses seem to range from 3 to 10. In this analysis we chose n=8. 

Finally we can combine the PCA and cross validation options within the  `trainControl` function such that:

```{r eval=FALSE}
tr<-trainControl(method="cv", number=8, preProcOptions="pca")
```

## Training Analysis




```{r eval=FALSE}
# Random Forrest
rfFit <-train(train$classe ~., method="rf", data=train, trControl=tr, verbose=FALSE)
rfPred=predict(rfFit,test)
print(rfPred)
```

```
[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```
```{r eval=FALSE}
# Generalized Boosted Regression Models
gbmFit  <- train(train$classe ~., method="gbm", data=train,trControl=tr,verbose=FALSE)
gbmPred=predict(gbmFit,test)
print(gbmPred)
```

```
[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```
```{r eval=FALSE}
# Support Vector Machines with Radial Basis Function Kernel
svmFit  <- train(train$classe ~., method="svmRadial", data=train,trControl=tr,verbose=FALSE)
svmPred=predict(svmFit,test)
print(svmPred)
```
```
 Loading required package: kernlab
 Attaching package: ‘kernlab’
 The following object is masked from ‘package:ggplot2’:
    alpha
```

```
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```


```{r eval=FALSE}
max(rfFit$results$Accuracy)
```

```
[1] 0.9984712
```

```{r eval=FALSE}
max(gbmFit$results$Accuracy)
```

```
[1] 0.9883294
```

```{r eval=FALSE}
max(svmFit$results$Accuracy))
```

```
[1] 0.9411886
```




