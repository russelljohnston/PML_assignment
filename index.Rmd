---
title: "Practical Machine Learning - Prediction Assignment"
author: "Russell Johnston"
date: "24 December 2015"
output: html_document
---
<style type="text/css">

body, td {
   font-size: 12px;
}
code.r{
  font-size: 10px;
}
pre {
  font-size: 8px
}
</style>

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har).

## The Goal

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

## Libraries and Settings

We use the following libraries to perfom our analysis.

```{r eval=FALSE}
library(caret)
library(randomForest)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(doParallel)

```

We set the seed to be 
```{r eval=FALSE}
set.seed(222)
```


## The Data

The training and testing data were respectively  downloaded from:

- [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

- [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

When reading in both datasets we standardise the data by setting any missing values  or `div/0`  to `NA` e.g.

```{r eval=FALSE}
data = read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
```


###  Sample Selection

The training set contains a total of 19622 observations and 160 variables. Not all of these are predictor variables and we firstly remove columns: `X` (row number), `user_name`, `raw_timestamp_part_1`, `raw_timestamp_part_2` and `cvtd_timestamp`:

```{r eval=FALSE}
data <- train[-c(1:6)]
```

Just for conveneince we move the outcome variable `classe` to the first column:
```{r eval=FALSE}
data <- data[c(154,1:153)]
```

We found that there are predictor columns with a significant number of missing data. Figure 1 below shows a bar plot of the percentage of NA counts for each variable in the training set:

```{r eval=FALSE}
na_count <-100*(sapply(data, function(y) sum(length(which(is.na(y)))))/nrow(data))
barplot(as.matrix(t(na_count)), las=3,cex.names=0.4,ylab="% of 'NA' counts")
```

<figure>
<img   style="margin:30px 0px 0px 0px;" src="barplot.png" width="500px"/>
<figcaption style="font-size:10px; margin:0px 0px 20px 0px"> Figure 1.Bar plot showing the percentage of NA counts for each variable in the training set. </figcaption>
</figure>
As we can see all variables with missing data have at least 97.93% without recorded values. That is,  these variables have at most, 406 recorded observations out of 19,622.


```{r eval=FALSE}
min(na_count[na_count>0])
[1] 97.93089
```
As such these variables would  have minimal contribution to the training analysis and so we remove these from our analysis:

```{r eval=FALSE}
data <- data[,(na_count==0)]
```

This results in the removal of 100 further variables from our anaylsis giving a total of 53 predictors:
```
 [1] "classe"               "num_window"           "roll_belt"            "pitch_belt"           "yaw_belt"           
 [6] "total_accel_belt"     "gyros_belt_x"         "gyros_belt_y"         "gyros_belt_z"         "accel_belt_x"       
[11] "accel_belt_y"         "accel_belt_z"         "magnet_belt_x"        "magnet_belt_y"        "magnet_belt_z"      
[16] "roll_arm"             "pitch_arm"            "yaw_arm"              "total_accel_arm"      "gyros_arm_x"        
[21] "gyros_arm_y"          "gyros_arm_z"          "accel_arm_x"          "accel_arm_y"          "accel_arm_z"        
[26] "magnet_arm_x"         "magnet_arm_y"         "magnet_arm_z"         "roll_dumbbell"        "pitch_dumbbell"     
[31] "yaw_dumbbell"         "total_accel_dumbbell" "gyros_dumbbell_x"     "gyros_dumbbell_y"     "gyros_dumbbell_z"   
[36] "accel_dumbbell_x"     "accel_dumbbell_y"     "accel_dumbbell_z"     "magnet_dumbbell_x"    "magnet_dumbbell_y"  
[41] "magnet_dumbbell_z"    "roll_forearm"         "pitch_forearm"        "yaw_forearm"          "total_accel_forearm"
[46] "gyros_forearm_x"      "gyros_forearm_y"      "gyros_forearm_z"      "accel_forearm_x"      "accel_forearm_y"    
[51] "accel_forearm_z"      "magnet_forearm_x"     "magnet_forearm_y"     "magnet_forearm_z"  
```


### Creating Training and Testing Data sets

We can now split the main `data` set into  training and testing samples by selecting 60% for the training and 40% for
the testing sample:

```{r eval=FALSE}
train <- createDataPartition(y=data$classe,p=.60,list=F)
training <- data[train,]
testing  <- data[-train,]
```
This will allow us to perform cross validation and estimate the out-of-samlple error. With this partition we have a total of 11776 observations for the `training` sample and 7846 in the `testing` sample.


### Pre processing with PCA and Cross Validation

#### Principle Component Analysis (PCA)
Since this is quite a high dimensional dataset we explore by how much the predictor variables are correlated.  We do this by applying the `cor` function and isolate variables that have a correlation coefficient > 0.8:

```{r eval=FALSE}
corrTrain <- abs(cor(training[,-1]))
diag(corrTrain) <- 0
which(corrTrain>0.8,arr.ind=T)
```
```
                 row col
yaw_belt           4   2
total_accel_belt   5   2
accel_belt_y      10   2
accel_belt_z      11   2
accel_belt_x       9   3
magnet_belt_x     12   3
roll_belt          2   4
roll_belt          2   5
accel_belt_y      10   5
accel_belt_z      11   5
pitch_belt         3   9
magnet_belt_x     12   9
roll_belt          2  10
total_accel_belt   5  10
accel_belt_z      11  10
roll_belt          2  11
total_accel_belt   5  11
accel_belt_y      10  11
pitch_belt         3  12
accel_belt_x       9  12
gyros_arm_y       20  19
gyros_arm_x       19  20
magnet_arm_x      25  22
accel_arm_x       22  25
magnet_arm_z      27  26
magnet_arm_y      26  27
accel_dumbbell_x  35  29
accel_dumbbell_z  37  30
pitch_dumbbell    29  35
yaw_dumbbell      30  37

```

We can see we have a number of  predictors that are highly correlated which would make principal component analysis (PCA) a useful addition to the analysis as it will reduce the number of predictors and help optimise the compuational cost. We will use the `caret` package to perform the training analysis and pass through pre processing options via the `trainControl` function. In terms of PCA we set  `preProcOptions="pca"` which, as default, retains 95% variance. 
This is equivalent to 27 components:

```{r eval=FALSE}
 preProcess(training, method = "pca", thresh = 0.95)$numComp
```
```
[1] 27
```
#### k-Fold Cross Validation

Implementing k-fold cross-validation will take the training sample and  randomly partition it into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The number of folds one chooses seem to range from 3 to 10. In this analysis we chose n=8. 

Finally we can combine the PCA and cross validation options within the  `trainControl` function such that:

```{r eval=FALSE}
tr<-trainControl(method="cv", number=5, preProcOptions="pca")
```

## Training Analysis

To help speed up the computations we envoke parallel processing prior to making our training model:
```{r eval=FALSE}

cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
```

We examine three different appraoches: random forrest `rf`, generalised boosted regresssion models `gbm` and support vector machines with radial basis function kernel  `svmRadial`. After training each model look apply it to our `testing` dataset and examine the resulting  `confusionMatrix`  to observe their respective accuracies.

```{r eval=FALSE}
# Random Forrest
rfFit <-train(training$classe ~., method="rf", data=training, trControl=tr, verbose=FALSE)
rfPred=predict(rfFit,testing)
rfConfMat = confusionMatrix(testing$classe,rfPred)
```

```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2230    1    0    0    1
         B    4 1513    1    0    0
         C    0    4 1364    0    0
         D    0    0    5 1281    0
         E    0    0    0    5 1437

Overall Statistics
                                          
               Accuracy : 0.9973          
                 95% CI : (0.9959, 0.9983)
    No Information Rate : 0.2847          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9966          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9982   0.9967   0.9956   0.9961   0.9993
Specificity            0.9996   0.9992   0.9994   0.9992   0.9992
Pos Pred Value         0.9991   0.9967   0.9971   0.9961   0.9965
Neg Pred Value         0.9993   0.9992   0.9991   0.9992   0.9998
Prevalence             0.2847   0.1935   0.1746   0.1639   0.1833
Detection Rate         0.2842   0.1928   0.1738   0.1633   0.1832
Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838
Balanced Accuracy      0.9989   0.9980   0.9975   0.9977   0.9993
```

```{r eval=FALSE}
# Generalized Boosted Regression Models
gbmFit  <- train(train$classe ~., method="gbm", data=train,trControl=tr,verbose=FALSE)
gbmPred=predict(gbmFit,testing)
gbmConfMat = confusionMatrix(testing$classe,gbmPred)
```

```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2222    9    0    1    0
         B   21 1493    3    1    0
         C    0   12 1353    1    2
         D    0    3   13 1266    4
         E    2    8    3   10 1419

Overall Statistics
                                          
               Accuracy : 0.9881          
                 95% CI : (0.9855, 0.9904)
    No Information Rate : 0.2861          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.985           
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9898   0.9790   0.9862   0.9898   0.9958
Specificity            0.9982   0.9960   0.9977   0.9970   0.9964
Pos Pred Value         0.9955   0.9835   0.9890   0.9844   0.9840
Neg Pred Value         0.9959   0.9949   0.9971   0.9980   0.9991
Prevalence             0.2861   0.1944   0.1749   0.1630   0.1816
Detection Rate         0.2832   0.1903   0.1724   0.1614   0.1809
Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838
Balanced Accuracy      0.9940   0.9875   0.9919   0.9934   0.9961

```


```{r eval=FALSE}
# Support Vector Machines with Radial Basis Function Kernel
svmFit  <- train(train$classe ~., method="svmRadial", data=train,trControl=tr,verbose=FALSE)
svmPred=predict(svmFit,testing)
svmConfMat = confusionMatrix(testing$classe,svmPred)
```

```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2183   13   32    1    3
         B  116 1360   41    0    1
         C    5   71 1266   21    5
         D    2    6  142 1134    2
         E    1    9   42   52 1338

Overall Statistics
                                         
               Accuracy : 0.928          
                 95% CI : (0.922, 0.9336)
    No Information Rate : 0.294          
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9088         
 Mcnemar's Test P-Value : < 2.2e-16      

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9463   0.9321   0.8313   0.9387   0.9918
Specificity            0.9912   0.9753   0.9839   0.9771   0.9840
Pos Pred Value         0.9780   0.8959   0.9254   0.8818   0.9279
Neg Pred Value         0.9779   0.9844   0.9603   0.9887   0.9983
Prevalence             0.2940   0.1860   0.1941   0.1540   0.1719
Detection Rate         0.2782   0.1733   0.1614   0.1445   0.1705
Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838
Balanced Accuracy      0.9687   0.9537   0.9076   0.9579   0.9879
> 

```
```{r eval=FALSE}
stopCluster(cl)
```

Comparing the three estimators we find the random forrest performs the best
shows a out of sample error of (1 - accuracy for predictions made against the cross-validation set) = 0.26%. 

Method |Accuracy |Out-of-sample Error (%)
------------- | ------------- | -------------
`rf` | 0.9973 | 0.26
`gbm` | 0.9881 | 1.19
`svmRadial` | 0.928 | 7.2


## Results of Testing on pml-testing Data

After finding the random forrest to be the most accurate of the three methods, we apply this to the final
sample of 20 observertions and make predictionst to be submittied to Coursera. We apply the same column selection to that the original training data to ensure the fitting model used in the later analysis throw no errors due to missing values:

```{r eval=FALSE}
T20 <- T20[-c(1:6)]
T20 <- T20[c(154,1:153)]
T20 <- T20[,(na_count==0)]
```

Now the predicitons:

```{r eval=FALSE}
predict(rfFit,T20)
```
```
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```

To submit the final predictions of the sample of 20, we use the code provided:

```{r eval=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```



## Conclusions

We have built a training model to using data from the Weight Lifting Exercise Dataset, which gathered data from data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. 

We split the data set into  training (60%) and testing (40%) samples and trained the data on three different machine leanring algorithms, random forrest `rf`, generalised boosted regresssion models `gbm` and support vector machines with radial basis function kernel  `svmRadial`. We found the random forrest to perform the best and applied this to the final test data of 20 samples and found that it performed well, finding 100% correct classifcation. This is perhaps not too surpriseing since the training model was shown to have ~99.7% accuracy. Since this data is trained to only 6 participants, it possibly is not representative  of a broader range of the population e.g. younger vs older, fitness level etc...  
